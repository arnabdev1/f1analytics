# ğŸï¸ F1 InsightHub â€” Machine Learning Model Overview  

Welcome to **F1 InsightHub**, an open-source Formula 1 analytics and prediction platform.  
This project combines ğŸ **Spring AI + PostgreSQL** (structured backend & chatbot), ğŸï¸ **Flask + MongoDB** (ML microservice), and ğŸ¨ **React + Tailwind** (frontend) to deliver interactive dashboards, predictions, and insights into car vs driver performance.  

---

## ğŸš¦ Tech Stack  
**Frontend:** React, TypeScript, Tailwind CSS  
**Backends:** Spring AI, Flask, REST APIs  
**Databases:** PostgreSQL, MongoDB  
**Machine Learning:** scikit-learn, pandas, numpy, joblib, SHAP  
**Other Tools:** Docker, OpenAI API, Kaggle datasets  

---

## ğŸ Overview of Features  
- ğŸ” **Secure User Login** with Spring Security (JWT & refresh tokens)  
- ğŸ“Š **Dashboard with live F1 standings & charts** via Ergast/Kaggle APIs  
- ğŸ¤– **AI Chatbot** powered by Spring AI + OpenAI API, enriched with ML model insights  
- ğŸ§  **Machine Learning predictions**: ranking constructors based on driver-adjusted performance  
- ğŸ—„ **Dual-database design**: PostgreSQL for structured F1 results, MongoDB for telemetry + ML metadata  

---

# ğŸ§  How the ML Model Works  

We use this Kaggle dataset for historic F1 data:  
ğŸ‘‰ [Formula 1 World Championship Dataset (1950â€“2020)](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020)  

### ğŸ“… Data Range  
We start analysis from **2001** (Fernando Alonsoâ€™s debut â€” the oldest active driver ğŸ).  

---

### ğŸ”¹ Asking About 2024  
- Kaggle contains **raw race results for 2024**.  
- Our ML pipeline does **not** just echo those results. Instead:  
  1. Train a **driver-only model** on 2001â€“2023 (predicts where drivers should finish *independent of cars*).  
  2. Use that model to predict 2024 finishes (based on skill & experience history).  
  3. Compare predictions vs real 2024 outcomes.  
  4. The difference = **car effect**.  
  5. Aggregate by constructor â†’ **car rankings** JSON.  

ğŸ‘‰ **So Haas can outrank Ferrari in our analysis** not because of championships, but because their drivers exceeded expectations relative to history â†’ a â€œcar boostâ€ effect.  

---

### ğŸ”¹ Why This Matters  
- Constructorsâ€™ standings = **car + driver combined**.  
- Our ML = **â€œhow much did the car help or hurt the driver?â€**  
- Example: Lewis Hamilton in a weaker Ferrari might underperform, while a rookie in McLaren could shine because of the carâ€™s strength.  

---

### ğŸ”¹ Projections for 2025  
- No Kaggle data yet â†’ no real outcomes.  
- Model trains on 2001â€“2024.  
- Predicts 2025 car rankings â†’ **pure forecasts**.  

---

### âš™ï¸ Technical Aspects  
- **Features:** qualifying positions, previous season stats, driver transfers, constructor points, reliability metrics, pit stop performance.  
- **Model:** Logistic Regression / XGBoost with driver/team embeddings.  
- **Evaluation:**  
  - Regression (finishing position RMSE/MAE).  
  - Ranking (Spearman correlation with constructor standings).  
  - Classification (podium probability ROC-AUC).  
- **Explainability:** SHAP values â†’ per-team explanations of why the model ranked them high/low.  
- **Persistence:** model artifacts stored with `joblib`, metadata & insights stored in MongoDB.  

---



## ğŸ› ï¸ Project Setup & Run Guide

Follow the steps below to run the project locally:

---

### â–¶ï¸ Frontend (React)

```bash
# Navigate to the frontend directory
cd frontend
# Install dependencies
npm install
# Run the development server
npm run dev
```

### â–¶ï¸ Backend (Flask)

```bash
# Navigate to the backend directory
cd backend-flask
# Run the development server
python3 -m venv venv
source venv/bin/activate
pip install pymongo flask_cors flask requests pandas scikit-learn statsmodels kaggle kagglehub joblib pyarrow

python3 app.py
```

